{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69f05d08-584d-474c-9aeb-f6c8244f0e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Initialize a SparkSession\n",
    "spark = SparkSession.builder.config(\"spark.jars\", \"postgresql-42.6.0.jar\").master('local[4]').getOrCreate()\n",
    "\n",
    "# slide 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1bf469e-4754-4d0e-958c-88e92c5a7a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Easy DF\n",
      "root\n",
      " |-- value: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/23 09:01:02 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "23/12/23 09:01:02 INFO DAGScheduler: Got job 4 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/12/23 09:01:02 INFO DAGScheduler: Final stage: ResultStage 4 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/12/23 09:01:02 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/12/23 09:01:02 INFO DAGScheduler: Missing parents: List()\n",
      "23/12/23 09:01:02 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[20] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/12/23 09:01:02 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 12.2 KiB, free 434.3 MiB)\n",
      "23/12/23 09:01:02 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 434.3 MiB)\n",
      "23/12/23 09:01:02 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 193.147.50.16:42587 (size: 6.5 KiB, free: 434.4 MiB)\n",
      "23/12/23 09:01:02 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1580\n",
      "23/12/23 09:01:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[20] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/12/23 09:01:02 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
      "23/12/23 09:01:02 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 8) (193.147.50.16, executor driver, partition 0, PROCESS_LOCAL, 7595 bytes) \n",
      "23/12/23 09:01:02 INFO Executor: Running task 0.0 in stage 4.0 (TID 8)\n",
      "23/12/23 09:01:02 INFO PythonRunner: Times: total = 46, boot = -49101, init = 49147, finish = 0\n",
      "23/12/23 09:01:02 INFO Executor: Finished task 0.0 in stage 4.0 (TID 8). 1843 bytes result sent to driver\n",
      "23/12/23 09:01:02 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 8) in 66 ms on 193.147.50.16 (executor driver) (1/1)\n",
      "23/12/23 09:01:02 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "23/12/23 09:01:02 INFO DAGScheduler: ResultStage 4 (showString at NativeMethodAccessorImpl.java:0) finished in 0.081 s\n",
      "23/12/23 09:01:02 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/12/23 09:01:02 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished\n",
      "23/12/23 09:01:02 INFO DAGScheduler: Job 4 finished: showString at NativeMethodAccessorImpl.java:0, took 0.089478 s\n",
      "23/12/23 09:01:02 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "23/12/23 09:01:02 INFO DAGScheduler: Got job 5 (showString at NativeMethodAccessorImpl.java:0) with 3 output partitions\n",
      "23/12/23 09:01:02 INFO DAGScheduler: Final stage: ResultStage 5 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/12/23 09:01:02 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/12/23 09:01:02 INFO DAGScheduler: Missing parents: List()\n",
      "23/12/23 09:01:02 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[20] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/12/23 09:01:02 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 12.2 KiB, free 434.3 MiB)\n",
      "23/12/23 09:01:02 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 434.3 MiB)\n",
      "23/12/23 09:01:02 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 193.147.50.16:42587 (size: 6.5 KiB, free: 434.4 MiB)\n",
      "23/12/23 09:01:02 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1580\n",
      "23/12/23 09:01:02 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 193.147.50.16:42587 in memory (size: 6.9 KiB, free: 434.4 MiB)\n",
      "23/12/23 09:01:02 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 5 (MapPartitionsRDD[20] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1, 2, 3))\n",
      "23/12/23 09:01:02 INFO TaskSchedulerImpl: Adding task set 5.0 with 3 tasks resource profile 0\n",
      "23/12/23 09:01:02 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 9) (193.147.50.16, executor driver, partition 1, PROCESS_LOCAL, 7624 bytes) \n",
      "23/12/23 09:01:02 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 193.147.50.16:42587 in memory (size: 6.5 KiB, free: 434.4 MiB)\n",
      "23/12/23 09:01:02 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 10) (193.147.50.16, executor driver, partition 2, PROCESS_LOCAL, 7624 bytes) \n",
      "23/12/23 09:01:02 INFO TaskSetManager: Starting task 2.0 in stage 5.0 (TID 11) (193.147.50.16, executor driver, partition 3, PROCESS_LOCAL, 7624 bytes) \n",
      "23/12/23 09:01:02 INFO Executor: Running task 2.0 in stage 5.0 (TID 11)\n",
      "23/12/23 09:01:02 INFO Executor: Running task 0.0 in stage 5.0 (TID 9)\n",
      "23/12/23 09:01:02 INFO Executor: Running task 1.0 in stage 5.0 (TID 10)\n",
      "23/12/23 09:01:02 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 193.147.50.16:42587 in memory (size: 6.9 KiB, free: 434.4 MiB)\n",
      "23/12/23 09:01:03 INFO PythonRunner: Times: total = 42, boot = -49217, init = 49259, finish = 0\n",
      "23/12/23 09:01:03 INFO PythonRunner: Times: total = 44, boot = -52, init = 96, finish = 0\n",
      "23/12/23 09:01:03 INFO PythonRunner: Times: total = 44, boot = -49222, init = 49266, finish = 0\n",
      "23/12/23 09:01:03 INFO Executor: Finished task 2.0 in stage 5.0 (TID 11). 1912 bytes result sent to driver\n",
      "23/12/23 09:01:03 INFO Executor: Finished task 1.0 in stage 5.0 (TID 10). 1912 bytes result sent to driver\n",
      "23/12/23 09:01:03 INFO Executor: Finished task 0.0 in stage 5.0 (TID 9). 1912 bytes result sent to driver\n",
      "23/12/23 09:01:03 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 10) in 76 ms on 193.147.50.16 (executor driver) (1/3)\n",
      "23/12/23 09:01:03 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 9) in 78 ms on 193.147.50.16 (executor driver) (2/3)\n",
      "23/12/23 09:01:03 INFO TaskSetManager: Finished task 2.0 in stage 5.0 (TID 11) in 75 ms on 193.147.50.16 (executor driver) (3/3)\n",
      "23/12/23 09:01:03 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "23/12/23 09:01:03 INFO DAGScheduler: ResultStage 5 (showString at NativeMethodAccessorImpl.java:0) finished in 0.104 s\n",
      "23/12/23 09:01:03 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/12/23 09:01:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished\n",
      "23/12/23 09:01:03 INFO DAGScheduler: Job 5 finished: showString at NativeMethodAccessorImpl.java:0, took 0.115527 s\n",
      "23/12/23 09:01:03 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "23/12/23 09:01:03 INFO DAGScheduler: Got job 6 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/12/23 09:01:03 INFO DAGScheduler: Final stage: ResultStage 6 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/12/23 09:01:03 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/12/23 09:01:03 INFO DAGScheduler: Missing parents: List()\n",
      "23/12/23 09:01:03 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[27] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/12/23 09:01:03 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 14.0 KiB, free 434.4 MiB)\n",
      "23/12/23 09:01:03 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 6.9 KiB, free 434.4 MiB)\n",
      "23/12/23 09:01:03 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 193.147.50.16:42587 (size: 6.9 KiB, free: 434.4 MiB)\n",
      "23/12/23 09:01:03 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1580\n",
      "23/12/23 09:01:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[27] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/12/23 09:01:03 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
      "23/12/23 09:01:03 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 12) (193.147.50.16, executor driver, partition 0, PROCESS_LOCAL, 7710 bytes) \n",
      "23/12/23 09:01:03 INFO Executor: Running task 0.0 in stage 6.0 (TID 12)\n",
      "23/12/23 09:01:03 INFO PythonRunner: Times: total = 42, boot = -138, init = 180, finish = 0\n",
      "23/12/23 09:01:03 INFO Executor: Finished task 0.0 in stage 6.0 (TID 12). 2064 bytes result sent to driver\n",
      "23/12/23 09:01:03 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 12) in 66 ms on 193.147.50.16 (executor driver) (1/1)\n",
      "23/12/23 09:01:03 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "23/12/23 09:01:03 INFO DAGScheduler: ResultStage 6 (showString at NativeMethodAccessorImpl.java:0) finished in 0.081 s\n",
      "23/12/23 09:01:03 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/12/23 09:01:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
      "23/12/23 09:01:03 INFO DAGScheduler: Job 6 finished: showString at NativeMethodAccessorImpl.java:0, took 0.087922 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    1|\n",
      "|    2|\n",
      "|    3|\n",
      "+-----+\n",
      "\n",
      "Complex DF\n",
      "root\n",
      " |-- _1: long (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: string (nullable = true)\n",
      " |-- _4: long (nullable = true)\n",
      " |-- _5: string (nullable = true)\n",
      " |-- _6: string (nullable = true)\n",
      "\n",
      "+---+------------+----------+---+---+-----------+\n",
      "| _1|          _2|        _3| _4| _5|         _6|\n",
      "+---+------------+----------+---+---+-----------+\n",
      "|  1|       Simón|   Bolivar| 47|VEN|489 895 965|\n",
      "|  2|       Fidel|    Castro| 90| CU|956 268 348|\n",
      "|  3|        Jose|   Doroteo| 45|MEX|985 621 444|\n",
      "|  4|     Ernesto|   Guevara| 39| AR|895 325 481|\n",
      "|  5|        Hugo|    Chávez| 58| VE|489 895 965|\n",
      "|  6|      Camilo|Cienfuegos| 27|CUB|956 268 348|\n",
      "|  7|    Emiliano|    Zapata| 39| ME|985 621 444|\n",
      "|  8|Juan Domingo|     Perón| 78|ARG|985 621 444|\n",
      "+---+------------+----------+---+---+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/23 09:01:03 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "23/12/23 09:01:03 INFO DAGScheduler: Got job 7 (showString at NativeMethodAccessorImpl.java:0) with 3 output partitions\n",
      "23/12/23 09:01:03 INFO DAGScheduler: Final stage: ResultStage 7 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/12/23 09:01:03 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/12/23 09:01:03 INFO DAGScheduler: Missing parents: List()\n",
      "23/12/23 09:01:03 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[27] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/12/23 09:01:03 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 14.0 KiB, free 434.3 MiB)\n",
      "23/12/23 09:01:03 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 6.9 KiB, free 434.3 MiB)\n",
      "23/12/23 09:01:03 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 193.147.50.16:42587 (size: 6.9 KiB, free: 434.4 MiB)\n",
      "23/12/23 09:01:03 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1580\n",
      "23/12/23 09:01:03 INFO DAGScheduler: Submitting 3 missing tasks from ResultStage 7 (MapPartitionsRDD[27] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(1, 2, 3))\n",
      "23/12/23 09:01:03 INFO TaskSchedulerImpl: Adding task set 7.0 with 3 tasks resource profile 0\n",
      "23/12/23 09:01:03 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 13) (193.147.50.16, executor driver, partition 1, PROCESS_LOCAL, 7711 bytes) \n",
      "23/12/23 09:01:03 INFO TaskSetManager: Starting task 1.0 in stage 7.0 (TID 14) (193.147.50.16, executor driver, partition 2, PROCESS_LOCAL, 7713 bytes) \n",
      "23/12/23 09:01:03 INFO TaskSetManager: Starting task 2.0 in stage 7.0 (TID 15) (193.147.50.16, executor driver, partition 3, PROCESS_LOCAL, 7706 bytes) \n",
      "23/12/23 09:01:03 INFO Executor: Running task 2.0 in stage 7.0 (TID 15)\n",
      "23/12/23 09:01:03 INFO Executor: Running task 0.0 in stage 7.0 (TID 13)\n",
      "23/12/23 09:01:03 INFO Executor: Running task 1.0 in stage 7.0 (TID 14)\n",
      "23/12/23 09:01:03 INFO PythonRunner: Times: total = 46, boot = -241, init = 287, finish = 0\n",
      "23/12/23 09:01:03 INFO PythonRunner: Times: total = 43, boot = -47, init = 90, finish = 0\n",
      "23/12/23 09:01:03 INFO PythonRunner: Times: total = 46, boot = -241, init = 287, finish = 0\n",
      "23/12/23 09:01:03 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 193.147.50.16:42587 in memory (size: 6.9 KiB, free: 434.4 MiB)\n",
      "23/12/23 09:01:03 INFO Executor: Finished task 2.0 in stage 7.0 (TID 15). 2101 bytes result sent to driver\n",
      "23/12/23 09:01:03 INFO Executor: Finished task 0.0 in stage 7.0 (TID 13). 2107 bytes result sent to driver\n",
      "23/12/23 09:01:03 INFO TaskSetManager: Finished task 2.0 in stage 7.0 (TID 15) in 86 ms on 193.147.50.16 (executor driver) (1/3)\n",
      "23/12/23 09:01:03 INFO Executor: Finished task 1.0 in stage 7.0 (TID 14). 2116 bytes result sent to driver\n",
      "23/12/23 09:01:03 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 13) in 90 ms on 193.147.50.16 (executor driver) (2/3)\n",
      "23/12/23 09:01:03 INFO TaskSetManager: Finished task 1.0 in stage 7.0 (TID 14) in 88 ms on 193.147.50.16 (executor driver) (3/3)\n",
      "23/12/23 09:01:03 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "23/12/23 09:01:03 INFO DAGScheduler: ResultStage 7 (showString at NativeMethodAccessorImpl.java:0) finished in 0.101 s\n",
      "23/12/23 09:01:03 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/12/23 09:01:03 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished\n",
      "23/12/23 09:01:03 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 193.147.50.16:42587 in memory (size: 6.5 KiB, free: 434.4 MiB)\n",
      "23/12/23 09:01:03 INFO DAGScheduler: Job 7 finished: showString at NativeMethodAccessorImpl.java:0, took 0.107278 s\n"
     ]
    }
   ],
   "source": [
    "# Some data with their column names. With DF we can structure our data\n",
    "columns = [\"id\",\"name\",\"surname\",\"age\",\"country\",\"local_phone\"]\n",
    "input_data = [(1,\"Simón\",\"Bolivar\",47,\"VEN\",\"489 895 965\"),\n",
    "    (2,\"Fidel\",\"Castro\",90,\"CU\",\"956 268 348\"),\n",
    "    (3,\"Jose\",\"Doroteo\",45,\"MEX\",\"985 621 444\"),\n",
    "    (4,\"Ernesto\",\"Guevara\",39,\"AR\",\"895 325 481\"),\n",
    "    (5,\"Hugo\",\"Chávez\",58,\"VE\",\"489 895 965\"),\n",
    "    (6,\"Camilo\",\"Cienfuegos\",27,\"CUB\",\"956 268 348\"),\n",
    "    (7,\"Emiliano\",\"Zapata\",39,\"ME\",\"985 621 444\"),\n",
    "    (8,\"Juan Domingo\",\"Perón\",78,\"ARG\",\"985 621 444\"),\n",
    "  ]\n",
    "\n",
    "# Simplier data\n",
    "int_list = [1,2,3]\n",
    "\n",
    "\n",
    "# intDF = spark.createDataFrame(int_list).toDF(\"value\") # this doesnt work\n",
    "# DF from primitive indicating type\n",
    "print(\"Easy DF\")\n",
    "intDF = spark.createDataFrame(int_list, \"int\").toDF(\"value\")\n",
    "intDF.printSchema()\n",
    "intDF.show()\n",
    "\n",
    "print(\"Complex DF\")\n",
    "complexDF = spark.createDataFrame(input_data)\n",
    "complexDF.printSchema()\n",
    "complexDF.show()\n",
    "\n",
    "# slide 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e988e106-f931-4437-9948-69bce083c397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: long (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: string (nullable = true)\n",
      " |-- _4: long (nullable = true)\n",
      " |-- _5: string (nullable = true)\n",
      " |-- _6: string (nullable = true)\n",
      "\n",
      "+---+------------+----------+---+---+-----------+\n",
      "| _1|          _2|        _3| _4| _5|         _6|\n",
      "+---+------------+----------+---+---+-----------+\n",
      "|  1|       Simón|   Bolivar| 47|VEN|489 895 965|\n",
      "|  2|       Fidel|    Castro| 90| CU|956 268 348|\n",
      "|  3|        Jose|   Doroteo| 45|MEX|985 621 444|\n",
      "|  4|     Ernesto|   Guevara| 39| AR|895 325 481|\n",
      "|  5|        Hugo|    Chávez| 58| VE|489 895 965|\n",
      "|  6|      Camilo|Cienfuegos| 27|CUB|956 268 348|\n",
      "|  7|    Emiliano|    Zapata| 39| ME|985 621 444|\n",
      "|  8|Juan Domingo|     Perón| 78|ARG|985 621 444|\n",
      "+---+------------+----------+---+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DF from RDD\n",
    "\n",
    "# Access the SparkContext object from the SparkSession object\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "# Create the DF from the RDD\n",
    "rdd = sc.parallelize(input_data)\n",
    "df = rdd.toDF()\n",
    "df.printSchema()\n",
    "\n",
    "df.show()\n",
    "\n",
    "# slide 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7a3e7cb-4084-4dee-9571-9b9d18e9a1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- surname: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- local_phone: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# You can pass the column names to the DF\n",
    "\n",
    "# Access the SparkContext object from the SparkSession object\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Create the DF from the RDD with column names\n",
    "rdd = sc.parallelize(input_data)\n",
    "\n",
    "columns = [\"id\",\"name\",\"surname\",\"age\",\"country\",\"local_phone\"]\n",
    "df = rdd.toDF(columns)\n",
    "df.printSchema()\n",
    "\n",
    "# slide 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3e09e41-b9a4-48e5-b659-dac4380b3767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- surname: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- local_phone: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Another method using RDD as input is the createDataFrame method from the\n",
    "# SparkSession object\n",
    "\n",
    "\n",
    "rdd = sc.parallelize(input_data)\n",
    "\n",
    "columns = [\"id\",\"name\",\"surname\",\"age\",\"country\",\"local_phone\"]\n",
    "df = spark.createDataFrame(rdd).toDF(*columns)\n",
    "df.printSchema()\n",
    "# The toDF method here is different because you are applying it to a DF not to an RDD,\n",
    "# check specifications on the documentation\n",
    "\n",
    "# slide 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dfbe7af-04bc-4a7f-84a0-a2720c7623de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- surname: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- local_phone: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using the Row type\n",
    "\n",
    "rowData = map(lambda x: Row(*x), input_data) \n",
    "df = spark.createDataFrame(rowData,columns)\n",
    "df.printSchema()\n",
    "\n",
    "# slide 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc568846-cc08-4f15-8321-90b457a6c64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- surname: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- local_phone: string (nullable = true)\n",
      "\n",
      "+---+------------+----------+---+-------+-----------+\n",
      "| id|        name|   surname|age|country|local_phone|\n",
      "+---+------------+----------+---+-------+-----------+\n",
      "|  1|       Simón|   Bolivar| 47|    VEN|489 895 965|\n",
      "|  2|       Fidel|    Castro| 90|     CU|956 268 348|\n",
      "|  3|        Jose|   Doroteo| 45|    MEX|985 621 444|\n",
      "|  4|     Ernesto|   Guevara| 39|     AR|895 325 481|\n",
      "|  5|        Hugo|    Chávez| 58|     VE|489 895 965|\n",
      "|  6|      Camilo|Cienfuegos| 27|    CUB|956 268 348|\n",
      "|  7|    Emiliano|    Zapata| 39|     ME|985 621 444|\n",
      "|  8|Juan Domingo|     Perón| 78|    ARG|985 621 444|\n",
      "+---+------------+----------+---+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating your schema using python pyspark.sql structures, most consistent approach\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([ \\\n",
    "    StructField(\"id\", IntegerType(), True), \\\n",
    "    StructField(\"name\",StringType(),True), \\\n",
    "    StructField(\"surname\",StringType(),True), \\\n",
    "    StructField(\"age\", StringType(), True), \\\n",
    "    StructField(\"country\",StringType(),True), \\\n",
    "    StructField(\"local_phone\", StringType(), True), \\\n",
    "  ])\n",
    "\n",
    "df = spark.createDataFrame(data=input_data,schema=schema)\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "# slide 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "822f5bb1-a029-4ae5-a942-7dc423dd3335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing CSV file\n",
    "\n",
    "output_path2 = \"../../data/csv_example/one_file\"\n",
    "output_path1 = \"../../data/csv_example/many_files\"\n",
    "\n",
    "# Write the DataFrame to CSV\n",
    "df.write.mode('overwrite').csv(output_path1, header=True)\n",
    "df.coalesce(1).write.mode('overwrite').csv(output_path2, header=True)\n",
    "\n",
    "# slide 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9807637-49c0-49f4-8bcd-2f55598797d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Read csv from file: \n",
      "\n",
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      "\n",
      "+---+------------+----------+---+-------+-----------+\n",
      "|_c0|         _c1|       _c2|_c3|    _c4|        _c5|\n",
      "+---+------------+----------+---+-------+-----------+\n",
      "| id|        name|   surname|age|country|local_phone|\n",
      "|  1|       Simón|   Bolivar| 47|    VEN|489 895 965|\n",
      "|  2|       Fidel|    Castro| 90|     CU|956 268 348|\n",
      "|  3|        Jose|   Doroteo| 45|    MEX|985 621 444|\n",
      "|  4|     Ernesto|   Guevara| 39|     AR|895 325 481|\n",
      "|  5|        Hugo|    Chávez| 58|     VE|489 895 965|\n",
      "|  6|      Camilo|Cienfuegos| 27|    CUB|956 268 348|\n",
      "|  7|    Emiliano|    Zapata| 39|     ME|985 621 444|\n",
      "|  8|Juan Domingo|     Perón| 78|    ARG|985 621 444|\n",
      "+---+------------+----------+---+-------+-----------+\n",
      "\n",
      "+ Read csv with header from file: \n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- surname: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- local_phone: string (nullable = true)\n",
      "\n",
      "+---+------------+----------+---+-------+-----------+\n",
      "| id|        name|   surname|age|country|local_phone|\n",
      "+---+------------+----------+---+-------+-----------+\n",
      "|  1|       Simón|   Bolivar| 47|    VEN|489 895 965|\n",
      "|  2|       Fidel|    Castro| 90|     CU|956 268 348|\n",
      "|  3|        Jose|   Doroteo| 45|    MEX|985 621 444|\n",
      "|  4|     Ernesto|   Guevara| 39|     AR|895 325 481|\n",
      "|  5|        Hugo|    Chávez| 58|     VE|489 895 965|\n",
      "|  6|      Camilo|Cienfuegos| 27|    CUB|956 268 348|\n",
      "|  7|    Emiliano|    Zapata| 39|     ME|985 621 444|\n",
      "|  8|Juan Domingo|     Perón| 78|    ARG|985 621 444|\n",
      "+---+------------+----------+---+-------+-----------+\n",
      "\n",
      "+ Read csv with header and infering data types from file: \n",
      "\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- surname: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- local_phone: string (nullable = true)\n",
      "\n",
      "+---+------------+----------+---+-------+-----------+\n",
      "| id|        name|   surname|age|country|local_phone|\n",
      "+---+------------+----------+---+-------+-----------+\n",
      "|  1|       Simón|   Bolivar| 47|    VEN|489 895 965|\n",
      "|  2|       Fidel|    Castro| 90|     CU|956 268 348|\n",
      "|  3|        Jose|   Doroteo| 45|    MEX|985 621 444|\n",
      "|  4|     Ernesto|   Guevara| 39|     AR|895 325 481|\n",
      "|  5|        Hugo|    Chávez| 58|     VE|489 895 965|\n",
      "|  6|      Camilo|Cienfuegos| 27|    CUB|956 268 348|\n",
      "|  7|    Emiliano|    Zapata| 39|     ME|985 621 444|\n",
      "|  8|Juan Domingo|     Perón| 78|    ARG|985 621 444|\n",
      "+---+------------+----------+---+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading CSV file\n",
    "\n",
    "read_path = '../../data/csv_example/one_file'\n",
    "\n",
    "print('+ Read csv from file: \\n')\n",
    "df_from_csv = spark.read.csv(read_path)\n",
    "df_from_csv.printSchema()\n",
    "df_from_csv.show()\n",
    "\n",
    "print('+ Read csv with header from file: \\n')\n",
    "df_from_csv = spark.read.csv(read_path,header=True)\n",
    "df_from_csv.printSchema()\n",
    "df_from_csv.show()\n",
    "\n",
    "print('+ Read csv with header and infering data types from file: \\n')\n",
    "df_from_csv = spark.read.csv(read_path,header=True, inferSchema=True)\n",
    "df_from_csv.printSchema()\n",
    "df_from_csv.show()\n",
    "\n",
    "# slide 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b44e50b-d88e-424d-9ed3-0c662915e4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving JSON file\n",
    "output_path1 = \"../../data/json_example/many_files\"\n",
    "output_path2 = \"../../data/json_example/one_file\"\n",
    "\n",
    "# Write the DataFrame to CSV\n",
    "df.write.mode('overwrite').json(output_path1)\n",
    "df.coalesce(1).write.mode('overwrite').option(\"multiline\",\"true\").json(output_path2)\n",
    "\n",
    "# slide 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfec3108-b62f-4f0b-a07c-3848d662d61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ Read json from file: \n",
      "\n",
      "root\n",
      " |-- age: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- local_phone: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- surname: string (nullable = true)\n",
      "\n",
      "+---+-------+---+-----------+------------+----------+\n",
      "|age|country| id|local_phone|        name|   surname|\n",
      "+---+-------+---+-----------+------------+----------+\n",
      "| 39|     ME|  7|985 621 444|    Emiliano|    Zapata|\n",
      "| 78|    ARG|  8|985 621 444|Juan Domingo|     Perón|\n",
      "| 58|     VE|  5|489 895 965|        Hugo|    Chávez|\n",
      "| 27|    CUB|  6|956 268 348|      Camilo|Cienfuegos|\n",
      "| 45|    MEX|  3|985 621 444|        Jose|   Doroteo|\n",
      "| 39|     AR|  4|895 325 481|     Ernesto|   Guevara|\n",
      "| 47|    VEN|  1|489 895 965|       Simón|   Bolivar|\n",
      "| 90|     CU|  2|956 268 348|       Fidel|    Castro|\n",
      "+---+-------+---+-----------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read data from json\n",
    "\n",
    "read_path = \"../../data/json_example/many_files\"\n",
    "\n",
    "print('+ Read json from file: \\n')\n",
    "df_from_json = spark.read.json(read_path)\n",
    "df_from_json.printSchema()\n",
    "df_from_json.show()\n",
    "\n",
    "# slide 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f369fc-99ba-422d-a11b-601f6af8b830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write in DB\n",
    "# You need to have a created DB on a local/remote postgresql server\n",
    "\n",
    "# Define database connection properties\n",
    "db_properties = {\n",
    "    \"url\": \"jdbc:postgresql://localhost:5432/pyspark_db\",\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "    \"user\": \"pyspark\",\n",
    "    \"password\": \"password\",\n",
    "}\n",
    "\n",
    "table_name = \"nice_guys\"\n",
    "\n",
    "# Write the DataFrame to the database\n",
    "df.write \\\n",
    "    .jdbc(url= db_properties[\"url\"],\n",
    "          table=table_name,\n",
    "          mode=\"overwrite\",  # You can use \"append\" or \"ignore\" as well\n",
    "          properties=db_properties)\n",
    "\n",
    "# slide 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc06857-72e2-4ffd-9fe4-0cba3c09077d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read DF from databse\n",
    "\n",
    "# Define database connection properties\n",
    "db_properties = {\n",
    "    \"url\": \"jdbc:postgresql://localhost:5432/pyspark_db\",\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "    \"user\": \"pyspark\",\n",
    "    \"password\": \"password\",\n",
    "}\n",
    "\n",
    "table_name = \"nice_guys\"\n",
    "\n",
    "df_from_postgresql = spark.read \\\n",
    "    .jdbc(url=db_properties[\"url\"],\n",
    "          table=\"nice_guys\",\n",
    "          properties=db_properties)\n",
    "\n",
    "df_from_postgresql.show()\n",
    "df = df_from_postgresql\n",
    "\n",
    "# slide 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ff0c530-f9de-4afb-b4b6-214bccceeae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+----------+---+-------+-----------+\n",
      "| id|        name|   surname|age|country|local_phone|\n",
      "+---+------------+----------+---+-------+-----------+\n",
      "|  7|    Emiliano|    Zapata| 39|     ME|985 621 444|\n",
      "|  8|Juan Domingo|     Perón| 78|    ARG|985 621 444|\n",
      "|  5|        Hugo|    Chávez| 58|     VE|489 895 965|\n",
      "|  6|      Camilo|Cienfuegos| 27|    CUB|956 268 348|\n",
      "|  3|        Jose|   Doroteo| 45|    MEX|985 621 444|\n",
      "|  4|     Ernesto|   Guevara| 39|     AR|895 325 481|\n",
      "|  1|       Simón|   Bolivar| 47|    VEN|489 895 965|\n",
      "|  2|       Fidel|    Castro| 90|     CU|956 268 348|\n",
      "+---+------------+----------+---+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load table with HIVE format to spark catalog to use it as view\n",
    "\n",
    "warehouse_location = spark.conf.get(\"spark.sql.warehouse.dir\")\n",
    "table_name = \"example_table\"\n",
    "\n",
    "df.write.mode('overwrite').parquet('../../data/parquet_example')\n",
    "\n",
    "# Load table\n",
    "df = spark.read.format('parquet').load('../../data/parquet_example')\n",
    "df.show()\n",
    "\n",
    "# slide 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7530c62c-07e9-43d0-a358-2f2dcaea5423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available tables:\n",
      "+  my_table\n"
     ]
    }
   ],
   "source": [
    "# Add table to spark catalog\n",
    "\n",
    "access_table_name = 'my_table'\n",
    "df.createOrReplaceTempView(access_table_name)\n",
    "\n",
    "print('Available tables:')\n",
    "tables = spark.catalog.listTables()\n",
    "for table in tables:\n",
    "    print('+ ',table.name)\n",
    "\n",
    "# slide 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27c76337-335d-41c9-88aa-f7505da17653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+----------+---+-------+-----------+\n",
      "| id|        name|   surname|age|country|local_phone|\n",
      "+---+------------+----------+---+-------+-----------+\n",
      "|  7|    Emiliano|    Zapata| 39|     ME|985 621 444|\n",
      "|  8|Juan Domingo|     Perón| 78|    ARG|985 621 444|\n",
      "|  5|        Hugo|    Chávez| 58|     VE|489 895 965|\n",
      "|  6|      Camilo|Cienfuegos| 27|    CUB|956 268 348|\n",
      "|  3|        Jose|   Doroteo| 45|    MEX|985 621 444|\n",
      "|  4|     Ernesto|   Guevara| 39|     AR|895 325 481|\n",
      "|  1|       Simón|   Bolivar| 47|    VEN|489 895 965|\n",
      "|  2|       Fidel|    Castro| 90|     CU|956 268 348|\n",
      "+---+------------+----------+---+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read some_table if its loaded in the spark.catalog\n",
    "\n",
    "df = spark.read.table(access_table_name)\n",
    "df.show()\n",
    "\n",
    "# slide 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e74fce3-94c4-4630-9644-07894ea2bc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+-------+---+-------+-----------+\n",
      "| id|        name|surname|age|country|local_phone|\n",
      "+---+------------+-------+---+-------+-----------+\n",
      "|  8|Juan Domingo|  Perón| 78|    ARG|985 621 444|\n",
      "|  5|        Hugo| Chávez| 58|     VE|489 895 965|\n",
      "|  3|        Jose|Doroteo| 45|    MEX|985 621 444|\n",
      "|  1|       Simón|Bolivar| 47|    VEN|489 895 965|\n",
      "|  2|       Fidel| Castro| 90|     CU|956 268 348|\n",
      "+---+------------+-------+---+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use SQL on DF\n",
    "\n",
    "access_table_name = 'my_table'\n",
    "result = spark.sql(f\"SELECT * FROM {access_table_name} WHERE Age >= 45\")\n",
    "result.show()\n",
    "\n",
    "# slide 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05ef89b4-f8f5-45b1-8488-83bd2a8d9fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+----------+---+-------+-----------+\n",
      "| id|    name|   surname|age|country|local_phone|\n",
      "+---+--------+----------+---+-------+-----------+\n",
      "|  7|Emiliano|    Zapata| 39|     ME|985 621 444|\n",
      "|  6|  Camilo|Cienfuegos| 27|    CUB|956 268 348|\n",
      "|  4| Ernesto|   Guevara| 39|     AR|895 325 481|\n",
      "+---+--------+----------+---+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.filter(col('age') < 45).show()\n",
    "\n",
    "# slide 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55f772d0-df7a-4004-8899-d930bb9ab623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+----------+---+-------+-----------+\n",
      "| id|        name|   surname|age|country|local_phone|\n",
      "+---+------------+----------+---+-------+-----------+\n",
      "|  1|       Simón|   Bolivar| 47|    VEN|489 895 965|\n",
      "|  2|       Fidel|    Castro| 90|     CU|956 268 348|\n",
      "|  3|        Jose|   Doroteo| 45|    MEX|985 621 444|\n",
      "|  4|     Ernesto|   Guevara| 39|     AR|895 325 481|\n",
      "|  5|        Hugo|    Chávez| 58|     VE|489 895 965|\n",
      "|  6|      Camilo|Cienfuegos| 27|    CUB|956 268 348|\n",
      "|  7|    Emiliano|    Zapata| 39|     ME|985 621 444|\n",
      "|  8|Juan Domingo|     Perón| 78|    ARG|985 621 444|\n",
      "+---+------------+----------+---+-------+-----------+\n",
      "\n",
      "+---+------------+----------+---+-------+-----------+\n",
      "| id|        name|   surname|age|country|local_phone|\n",
      "+---+------------+----------+---+-------+-----------+\n",
      "|  6|      Camilo|Cienfuegos| 27|    CUB|956 268 348|\n",
      "|  4|     Ernesto|   Guevara| 39|     AR|895 325 481|\n",
      "|  7|    Emiliano|    Zapata| 39|     ME|985 621 444|\n",
      "|  3|        Jose|   Doroteo| 45|    MEX|985 621 444|\n",
      "|  1|       Simón|   Bolivar| 47|    VEN|489 895 965|\n",
      "|  5|        Hugo|    Chávez| 58|     VE|489 895 965|\n",
      "|  8|Juan Domingo|     Perón| 78|    ARG|985 621 444|\n",
      "|  2|       Fidel|    Castro| 90|     CU|956 268 348|\n",
      "+---+------------+----------+---+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Order by\n",
    "\n",
    "df.orderBy('id').show()\n",
    "\n",
    "df.orderBy('age').show()\n",
    "\n",
    "# slide 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e2108b8-2e6c-41a2-ae3b-6acf4e39fe01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---+-------+-----------+\n",
      "| id|   surname|age|country|local_phone|\n",
      "+---+----------+---+-------+-----------+\n",
      "|  7|    Zapata| 39|     ME|985 621 444|\n",
      "|  8|     Perón| 78|    ARG|985 621 444|\n",
      "|  5|    Chávez| 58|     VE|489 895 965|\n",
      "|  6|Cienfuegos| 27|    CUB|956 268 348|\n",
      "|  3|   Doroteo| 45|    MEX|985 621 444|\n",
      "|  4|   Guevara| 39|     AR|895 325 481|\n",
      "|  1|   Bolivar| 47|    VEN|489 895 965|\n",
      "|  2|    Castro| 90|     CU|956 268 348|\n",
      "+---+----------+---+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop\n",
    "\n",
    "df.drop('name').show()\n",
    "\n",
    "# slide 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fcfb235e-c06a-48f9-b963-5c6939b260a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+----------+---+-------+-----------+-----+\n",
      "| id|        name|   surname|age|country|local_phone|young|\n",
      "+---+------------+----------+---+-------+-----------+-----+\n",
      "|  7|    Emiliano|    Zapata| 39|     ME|985 621 444| true|\n",
      "|  8|Juan Domingo|     Perón| 78|    ARG|985 621 444| true|\n",
      "|  5|        Hugo|    Chávez| 58|     VE|489 895 965| true|\n",
      "|  6|      Camilo|Cienfuegos| 27|    CUB|956 268 348|false|\n",
      "|  3|        Jose|   Doroteo| 45|    MEX|985 621 444| true|\n",
      "|  4|     Ernesto|   Guevara| 39|     AR|895 325 481| true|\n",
      "|  1|       Simón|   Bolivar| 47|    VEN|489 895 965| true|\n",
      "|  2|       Fidel|    Castro| 90|     CU|956 268 348| true|\n",
      "+---+------------+----------+---+-------+-----------+-----+\n",
      "\n",
      "+---+------------+----------+---+-------+-----------+-----+\n",
      "| id|        name|   surname|age|country|local_phone|young|\n",
      "+---+------------+----------+---+-------+-----------+-----+\n",
      "|  7|    Emiliano|    Zapata| 39|     ME|985 621 444| true|\n",
      "|  8|Juan Domingo|     Perón| 78|    ARG|985 621 444| true|\n",
      "|  5|        Hugo|    Chávez| 58|     VE|489 895 965| true|\n",
      "|  6|      Camilo|Cienfuegos| 27|    CUB|956 268 348|false|\n",
      "|  3|        Jose|   Doroteo| 45|    MEX|985 621 444| true|\n",
      "|  4|     Ernesto|   Guevara| 39|     AR|895 325 481| true|\n",
      "|  1|       Simón|   Bolivar| 47|    VEN|489 895 965| true|\n",
      "|  2|       Fidel|    Castro| 90|     CU|956 268 348| true|\n",
      "+---+------------+----------+---+-------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add column to DF\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "new_df = df.withColumn('young', col('age') > 30)\n",
    "new_df.show()\n",
    "\n",
    "# SQL equivalent...\n",
    "access_table_name = 'my_table'\n",
    "spark.sql(f\"SELECT *,(Age > 30) AS young FROM {access_table_name}\").show()\n",
    "\n",
    "# slide 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "480f51fc-f2bc-41cc-a695-8df27f0e2ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+----------+---+-------+-----------+----------+\n",
      "| id|        name|   surname|age|country|local_phone|phone_code|\n",
      "+---+------------+----------+---+-------+-----------+----------+\n",
      "|  1|       Simón|   Bolivar| 47|    VEN|489 895 965|       +58|\n",
      "|  2|       Fidel|    Castro| 90|     CU|956 268 348|       +53|\n",
      "|  3|        Jose|   Doroteo| 45|    MEX|985 621 444|       +52|\n",
      "|  4|     Ernesto|   Guevara| 39|     AR|895 325 481|       +54|\n",
      "|  5|        Hugo|    Chávez| 58|     VE|489 895 965|       +58|\n",
      "|  6|      Camilo|Cienfuegos| 27|    CUB|956 268 348|       +53|\n",
      "|  7|    Emiliano|    Zapata| 39|     ME|985 621 444|       +52|\n",
      "|  8|Juan Domingo|     Perón| 78|    ARG|985 621 444|       +54|\n",
      "+---+------------+----------+---+-------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = [\"id\",\"name\",\"surname\",\"age\",\"country\",\"local_phone\",\"phone_code\"]\n",
    "input_data = [(1,\"Simón\",\"Bolivar\",47,\"VEN\",\"489 895 965\",\"+58\"),\n",
    "    (2,\"Fidel\",\"Castro\",90,\"CU\",\"956 268 348\",\"+53\"),\n",
    "    (3,\"Jose\",\"Doroteo\",45,\"MEX\",\"985 621 444\",\"+52\"),\n",
    "    (4,\"Ernesto\",\"Guevara\",39,\"AR\",\"895 325 481\",\"+54\"),\n",
    "    (5,\"Hugo\",\"Chávez\",58,\"VE\",\"489 895 965\",\"+58\"),\n",
    "    (6,\"Camilo\",\"Cienfuegos\",27,\"CUB\",\"956 268 348\",\"+53\"),\n",
    "    (7,\"Emiliano\",\"Zapata\",39,\"ME\",\"985 621 444\",\"+52\"),\n",
    "    (8,\"Juan Domingo\",\"Perón\",78,\"ARG\",\"985 621 444\",\"+54\"),\n",
    "  ]\n",
    "\n",
    "mod_df = spark.createDataFrame(input_data).toDF(*columns)\n",
    "mod_df.show()\n",
    "\n",
    "# slide 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be794c1c-b67c-4a40-93aa-8938fe92a7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|phone_code|item_count|\n",
      "+----------+----------+\n",
      "|       +58|         2|\n",
      "|       +53|         2|\n",
      "|       +54|         2|\n",
      "|       +52|         2|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grouping and Aggregation\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "result = mod_df.groupBy(\"phone_code\") \\\n",
    "                .agg(count(\"*\").alias(\"item_count\"))\n",
    "result.show()\n",
    "\n",
    "# slide 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9537a83-399c-452b-ab8f-bc450d8a3994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+------+\n",
      "|phone_code|item_count|   ids|\n",
      "+----------+----------+------+\n",
      "|       +58|         2|[1, 5]|\n",
      "|       +53|         2|[2, 6]|\n",
      "|       +54|         2|[4, 8]|\n",
      "|       +52|         2|[3, 7]|\n",
      "+----------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Grouping and Aggregation 2 columns\n",
    "\n",
    "from pyspark.sql.functions import collect_list, count\n",
    "\n",
    "result = mod_df.groupBy(\"phone_code\") \\\n",
    "                .agg(count(\"*\").alias(\"item_count\"), \\\n",
    "                     collect_list(\"id\").alias(\"ids\"))\n",
    "result.show()\n",
    "\n",
    "# slide 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b700f520-5d50-4e5c-82ed-78d221fd8353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|phone_code|age_avg|\n",
      "+----------+-------+\n",
      "|       +58|   52.5|\n",
      "|       +53|   58.5|\n",
      "|       +54|   58.5|\n",
      "|       +52|   42.0|\n",
      "+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Average\n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "result = mod_df.groupBy(\"phone_code\") \\\n",
    "                .agg(avg('age').alias(\"age_avg\"))\n",
    "\n",
    "result.show()\n",
    "\n",
    "# slide 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e3f24d92-a18c-4dfe-8a7a-16373956bc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+----------+---+-------+-----------+----------+-----+\n",
      "| id|        name|   surname|age|country|local_phone|phone_code|young|\n",
      "+---+------------+----------+---+-------+-----------+----------+-----+\n",
      "|  1|       Simón|   Bolivar| 47|    VEN|489 895 965|       +58| true|\n",
      "|  2|       Fidel|    Castro| 90|     CU|956 268 348|       +53| true|\n",
      "|  3|        Jose|   Doroteo| 45|    MEX|985 621 444|       +52| true|\n",
      "|  4|     Ernesto|   Guevara| 39|     AR|895 325 481|       +54| true|\n",
      "|  5|        Hugo|    Chávez| 58|     VE|489 895 965|       +58| true|\n",
      "|  6|      Camilo|Cienfuegos| 27|    CUB|956 268 348|       +53|false|\n",
      "|  7|    Emiliano|    Zapata| 39|     ME|985 621 444|       +52| true|\n",
      "|  8|Juan Domingo|     Perón| 78|    ARG|985 621 444|       +54| true|\n",
      "+---+------------+----------+---+-------+-----------+----------+-----+\n",
      "\n",
      "+-----+-----------------------------------+\n",
      "|young|phone_codes                        |\n",
      "+-----+-----------------------------------+\n",
      "|true |[+58, +53, +52, +54, +58, +52, +54]|\n",
      "|false|[+53]                              |\n",
      "+-----+-----------------------------------+\n",
      "\n",
      "+-----+--------------------+\n",
      "|young|phone_codes         |\n",
      "+-----+--------------------+\n",
      "|true |[+52, +58, +54, +53]|\n",
      "|false|[+53]               |\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Collect list, collect set\n",
    "\n",
    "from pyspark.sql.functions import collect_list, collect_set, col\n",
    "\n",
    "df = mod_df.withColumn('young', col('age') > 30)\n",
    "df.show()\n",
    "\n",
    "\n",
    "result = df.groupBy(\"young\") \\\n",
    "                .agg(\n",
    "                      collect_list(\"phone_code\").alias(\"phone_codes\"))\n",
    "result.show(truncate=False)\n",
    "\n",
    "result = df.groupBy(\"young\") \\\n",
    "                .agg(\n",
    "                      collect_set(\"phone_code\").alias(\"phone_codes\"))\n",
    "result.show(truncate=False)\n",
    "\n",
    "\n",
    "# slide 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ba6710ac-2ea5-4f84-8506-bed29ef74d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------------+\n",
      "|phone_code|item_count|               names|\n",
      "+----------+----------+--------------------+\n",
      "|       +58|         2|       [Simón, Hugo]|\n",
      "|       +53|         2|     [Fidel, Camilo]|\n",
      "|       +54|         2|[Ernesto, Juan Do...|\n",
      "|       +52|         2|    [Jose, Emiliano]|\n",
      "+----------+----------+--------------------+\n",
      "\n",
      "+----------+------------+\n",
      "|phone_code|        name|\n",
      "+----------+------------+\n",
      "|       +58|       Simón|\n",
      "|       +58|        Hugo|\n",
      "|       +53|       Fidel|\n",
      "|       +53|      Camilo|\n",
      "|       +54|     Ernesto|\n",
      "|       +54|Juan Domingo|\n",
      "|       +52|        Jose|\n",
      "|       +52|    Emiliano|\n",
      "+----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explode example\n",
    "\n",
    "from pyspark.sql.functions import explode, collect_list, count\n",
    "\n",
    "result = mod_df.groupBy(\"phone_code\") \\\n",
    "                .agg(count(\"*\").alias(\"item_count\"), \\\n",
    "                     collect_list(\"name\").alias(\"names\"))\n",
    "result.show()\n",
    "\n",
    "result = result.select('phone_code', explode('names').alias('name'))\n",
    "\n",
    "result.show()\n",
    "\n",
    "\n",
    "# slide 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bd7f8dff-c737-44c9-9ab3-790bf08657e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- event: string (nullable = true)\n",
      "\n",
      "+---+--------------------+--------------------+\n",
      "| id|               event|           timestamp|\n",
      "+---+--------------------+--------------------+\n",
      "|  1|2023-09-23 05:23:...|2023-09-23 05:23:...|\n",
      "+---+--------------------+--------------------+\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- event: string (nullable = true)\n",
      "\n",
      "+---+--------------------+--------------------+\n",
      "| id|               event|           timestamp|\n",
      "+---+--------------------+--------------------+\n",
      "|  1|23-09-2023 05:23:...|2023-09-23 05:23:...|\n",
      "+---+--------------------+--------------------+\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- event: string (nullable = true)\n",
      "\n",
      "+---+--------------------+---------+\n",
      "| id|               event|timestamp|\n",
      "+---+--------------------+---------+\n",
      "|  1|23-09-2023 05:23:...|     NULL|\n",
      "+---+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Timestamps\n",
    "\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "df=spark.createDataFrame(\n",
    "        data = [ (\"1\",\"2023-09-23 05:23:02.013\")],\n",
    "        schema=[\"id\",\"event\"])\n",
    "df.printSchema()\n",
    "\n",
    "#Timestamp String to DateType\n",
    "df.withColumn(\"timestamp\",to_timestamp(\"event\")) \\\n",
    "  .show()\n",
    "\n",
    "# format 'yyyy-MM-dd  HH:mm:ss.SSS'\n",
    "\n",
    "# Custom format\n",
    "df=spark.createDataFrame(\n",
    "        data = [ (\"1\",\"23-09-2023 05:23:02.013\")],\n",
    "        schema=[\"id\",\"event\"])\n",
    "df.printSchema()\n",
    "\n",
    "#Timestamp String to DateType\n",
    "df.withColumn(\"timestamp\",to_timestamp(\"event\",'dd-MM-yyyy HH:mm:ss.SSSS')) \\\n",
    "  .show()\n",
    "\n",
    "# Custom format, be careful things can go wrong\n",
    "df=spark.createDataFrame(\n",
    "        data = [ (\"1\",\"23-09-2023 05:23:02.013\")],\n",
    "        schema=[\"id\",\"event\"])\n",
    "df.printSchema()\n",
    "\n",
    "#Timestamp String to DateType\n",
    "df.withColumn(\"timestamp\",to_timestamp(\"event\",'MM-dd-yyyy HH:mm:ss.SSSS')) \\\n",
    "  .show()\n",
    "\n",
    "# slide 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2e75e586-98de-49f0-a771-c571c4020e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------+\n",
      "|      date|current_date|datediff|\n",
      "+----------+------------+--------+\n",
      "|2023-09-28|  2023-12-23|      86|\n",
      "|2001-09-11|  2023-12-23|    8138|\n",
      "|1989-11-09|  2023-12-23|   12462|\n",
      "+----------+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Time differences\n",
    "\n",
    "from pyspark.sql.functions import current_date, datediff, col\n",
    "\n",
    "data = [(\"1\",\"2023-09-28\"),(\"2\",\"2001-09-11\"),(\"3\",\"1989-11-09\")]\n",
    "df=spark.createDataFrame(data=data,schema=[\"id\",\"date\"])\n",
    "\n",
    "df.select(\n",
    "      col(\"date\"),\n",
    "      current_date().alias(\"current_date\"),\n",
    "      datediff(current_date(),col(\"date\")).alias(\"datediff\")\n",
    "    ).show()\n",
    "\n",
    "#slide 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6ff22e3a-bbf1-43fc-8794-4da29c3988e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+----------+---+-------+-----------+----------+\n",
      "| id|        name|   surname|age|country|local_phone|phone_code|\n",
      "+---+------------+----------+---+-------+-----------+----------+\n",
      "|  6|      Camilo|Cienfuegos| 27|    CUB|956 268 348|       +53|\n",
      "|  4|     Ernesto|   Guevara| 39|     AR|895 325 481|       +54|\n",
      "|  7|    Emiliano|    Zapata| 39|     ME|985 621 444|       +52|\n",
      "|  3|        Jose|   Doroteo| 45|    MEX|985 621 444|       +52|\n",
      "|  1|       Simón|   Bolivar| 47|    VEN|489 895 965|       +58|\n",
      "|  5|        Hugo|    Chávez| 58|     VE|489 895 965|       +58|\n",
      "|  8|Juan Domingo|     Perón| 78|    ARG|985 621 444|       +54|\n",
      "|  2|       Fidel|    Castro| 90|     CU|956 268 348|       +53|\n",
      "+---+------------+----------+---+-------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sorting\n",
    "\n",
    "from pyspark.sql.functions import asc_nulls_first\n",
    "\n",
    "result = mod_df.orderBy(mod_df.age.asc_nulls_first())\n",
    "result.show()\n",
    "# You will get null values first\n",
    "\n",
    "#slide 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c1494f5e-1631-4103-ae91-bdd25deb1979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+----------+---+-------+-----------+----------+\n",
      "| id|        name|   surname|age|country|local_phone|phone_code|\n",
      "+---+------------+----------+---+-------+-----------+----------+\n",
      "|  1|       Simón|   Bolivar| 47|    VEN|489 895 965|        58|\n",
      "|  2|       Fidel|    Castro| 90|     CU|956 268 348|        53|\n",
      "|  3|        Jose|   Doroteo| 45|    MEX|985 621 444|        52|\n",
      "|  4|     Ernesto|   Guevara| 39|     AR|895 325 481|        54|\n",
      "|  5|        Hugo|    Chávez| 58|     VE|489 895 965|        58|\n",
      "|  6|      Camilo|Cienfuegos| 27|    CUB|956 268 348|        53|\n",
      "|  7|    Emiliano|    Zapata| 39|     ME|985 621 444|        52|\n",
      "|  8|Juan Domingo|     Perón| 78|    ARG|985 621 444|        54|\n",
      "+---+------------+----------+---+-------+-----------+----------+\n",
      "\n",
      "+---+------------+----------+---+-------+-----------+----------+\n",
      "| id|        name|   surname|age|country|local_phone|phone_code|\n",
      "+---+------------+----------+---+-------+-----------+----------+\n",
      "|  1|       Simón|   Bolivar| 47|    VEN|  489895965|       +58|\n",
      "|  2|       Fidel|    Castro| 90|     CU|  956268348|       +53|\n",
      "|  3|        Jose|   Doroteo| 45|    MEX|  985621444|       +52|\n",
      "|  4|     Ernesto|   Guevara| 39|     AR|  895325481|       +54|\n",
      "|  5|        Hugo|    Chávez| 58|     VE|  489895965|       +58|\n",
      "|  6|      Camilo|Cienfuegos| 27|    CUB|  956268348|       +53|\n",
      "|  7|    Emiliano|    Zapata| 39|     ME|  985621444|       +52|\n",
      "|  8|Juan Domingo|     Perón| 78|    ARG|  985621444|       +54|\n",
      "+---+------------+----------+---+-------+-----------+----------+\n",
      "\n",
      "+---+------------+----------+---+-------+------------+\n",
      "| id|        name|   surname|age|country|phone_number|\n",
      "+---+------------+----------+---+-------+------------+\n",
      "|  1|       SIMÓN|   BOLIVAR| 47|    VEN| 58489895965|\n",
      "|  2|       FIDEL|    CASTRO| 90|     CU| 53956268348|\n",
      "|  3|        JOSE|   DOROTEO| 45|    MEX| 52985621444|\n",
      "|  4|     ERNESTO|   GUEVARA| 39|     AR| 54895325481|\n",
      "|  5|        HUGO|    CHÁVEZ| 58|     VE| 58489895965|\n",
      "|  6|      CAMILO|CIENFUEGOS| 27|    CUB| 53956268348|\n",
      "|  7|    EMILIANO|    ZAPATA| 39|     ME| 52985621444|\n",
      "|  8|JUAN DOMINGO|     PERÓN| 78|    ARG| 54985621444|\n",
      "+---+------------+----------+---+-------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using regular expersions\n",
    "\n",
    "from pyspark.sql.functions import regexp_extract, regexp_replace, concat_ws, upper\n",
    "\n",
    "prefix_pattern = r'(\\d+)'\n",
    "# Use regexp_extract to extract the prefix from the \"local_phone\" column\n",
    "df1 = mod_df.withColumn(\"phone_code\", regexp_extract(col(\"phone_code\"), prefix_pattern, 1))\n",
    "df1.show()\n",
    "\n",
    "df2 = mod_df.withColumn(\"local_phone\", regexp_replace(col(\"local_phone\"), r'\\s+', ''))\n",
    "df2.show()\n",
    "\n",
    "df3 = mod_df.withColumn(\"phone_number\", concat_ws(\"\", regexp_extract(col(\"phone_code\"), prefix_pattern, 1), \n",
    "                                                  regexp_replace(col(\"local_phone\"), r'\\s+', ''))) \\\n",
    "            .drop('phone_code','local_phone') \\\n",
    "            .withColumn(\"name\", upper(col(\"name\"))) \\\n",
    "            .withColumn(\"surname\", upper(col(\"surname\")))\n",
    "df3.show()\n",
    "\n",
    "\n",
    "#slide 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "193113b4-203a-45f5-a409-5fe83eb467b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- x: long (nullable = false)\n",
      " |-- is_even: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @udf(returnType=StringType())\n",
    "def is_even_udf(x):\n",
    "    return x % 2 == 0\n",
    "\n",
    "df = spark.range(1, 100).toDF(\"x\")\n",
    "\n",
    "# is_even_udf = udf(is_even_udf, StringType())\n",
    "\n",
    "result_df = df.select(col(\"x\"), is_even_udf(col(\"x\")) \\\n",
    ".alias(\"is_even\"))\n",
    "\n",
    "result_df.printSchema()\n",
    "\n",
    "#slide 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f75a0a-5ed1-4060-a0bf-0a1fbb8adb7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# UDAF, using pandas...\n",
    "\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Define a custom UDAF to calculate the average age per country\n",
    "@pandas_udf(DoubleType(), PandasUDFType.GROUPED_AGG)\n",
    "def average_age_udaf(v):\n",
    "    return v.mean()\n",
    "\n",
    "# Use the UDAF to calculate average age per country\n",
    "result = df.groupBy(\"country\").agg(average_age_udaf(col(\"age\")).alias(\"average_age\"))\n",
    "\n",
    "# Show the result\n",
    "result.show()\n",
    "\n",
    "# slide 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "15e8359d-1061-4916-a4cc-d7f13f01030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd99e339",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
