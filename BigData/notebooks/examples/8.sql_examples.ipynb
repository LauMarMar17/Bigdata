{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f05d08-584d-474c-9aeb-f6c8244f0e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Initialize a SparkSession\n",
    "spark = SparkSession.builder.config(\"spark.jars\", \"postgresql-42.6.0.jar\").master('local[4]').getOrCreate()\n",
    "\n",
    "# slide 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bf469e-4754-4d0e-958c-88e92c5a7a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some data with their column names. With DF we can structure our data\n",
    "columns = [\"id\",\"name\",\"surname\",\"age\",\"country\",\"local_phone\"]\n",
    "input_data = [(1,\"Simón\",\"Bolivar\",47,\"VEN\",\"489 895 965\"),\n",
    "    (2,\"Fidel\",\"Castro\",90,\"CU\",\"956 268 348\"),\n",
    "    (3,\"Jose\",\"Doroteo\",45,\"MEX\",\"985 621 444\"),\n",
    "    (4,\"Ernesto\",\"Guevara\",39,\"AR\",\"895 325 481\"),\n",
    "    (5,\"Hugo\",\"Chávez\",58,\"VE\",\"489 895 965\"),\n",
    "    (6,\"Camilo\",\"Cienfuegos\",27,\"CUB\",\"956 268 348\"),\n",
    "    (7,\"Emiliano\",\"Zapata\",39,\"ME\",\"985 621 444\"),\n",
    "    (8,\"Juan Domingo\",\"Perón\",78,\"ARG\",\"985 621 444\"),\n",
    "  ]\n",
    "\n",
    "# Simplier data\n",
    "int_list = [1,2,3]\n",
    "\n",
    "\n",
    "# intDF = spark.createDataFrame(int_list).toDF(\"value\") # this doesnt work\n",
    "# DF from primitive indicating type\n",
    "intDF = spark.createDataFrame(int_list, \"int\").toDF(\"value\")\n",
    "intDF.printSchema()\n",
    "intDF.show()\n",
    "\n",
    "complexDF = spark.createDataFrame(input_data)\n",
    "complexDF.printSchema()\n",
    "complexDF.show()\n",
    "\n",
    "# slide 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e988e106-f931-4437-9948-69bce083c397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF from RDD\n",
    "\n",
    "# Access the SparkContext object from the SparkSession object\n",
    "sc = spark.sparkContext\n",
    "# Create the DF from the RDD\n",
    "rdd = sc.parallelize(input_data)\n",
    "df = rdd.toDF()\n",
    "df.printSchema()\n",
    "\n",
    "# slide 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a3e7cb-4084-4dee-9571-9b9d18e9a1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can pass the column names to the DF\n",
    "\n",
    "# Access the SparkContext object from the SparkSession object\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Create the DF from the RDD with column names\n",
    "rdd = sc.parallelize(input_data)\n",
    "\n",
    "columns = [\"id\",\"name\",\"surname\",\"age\",\"country\",\"local_phone\"]\n",
    "df = rdd.toDF(columns)\n",
    "df.printSchema()\n",
    "\n",
    "# slide 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e09e41-b9a4-48e5-b659-dac4380b3767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another method using RDD as input is the createDataFrame method from the\n",
    "# SparkSession object\n",
    "\n",
    "\n",
    "rdd = sc.parallelize(input_data)\n",
    "\n",
    "columns = [\"id\",\"name\",\"surname\",\"age\",\"country\",\"local_phone\"]\n",
    "df = spark.createDataFrame(rdd).toDF(*columns)\n",
    "df.printSchema()\n",
    "# The toDF method here is different because you are applying it to a DF not to an RDD,\n",
    "# check specifications on the documentation\n",
    "\n",
    "# slide 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfbe7af-04bc-4a7f-84a0-a2720c7623de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the Row type\n",
    "\n",
    "rowData = map(lambda x: Row(*x), input_data) \n",
    "df = spark.createDataFrame(rowData,columns)\n",
    "df.printSchema()\n",
    "\n",
    "# slide 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc568846-cc08-4f15-8321-90b457a6c64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating your schema using python pyspark.sql structures, most consistent approach\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([ \\\n",
    "    StructField(\"id\", IntegerType(), True), \\\n",
    "    StructField(\"name\",StringType(),True), \\\n",
    "    StructField(\"surname\",StringType(),True), \\\n",
    "    StructField(\"age\", StringType(), True), \\\n",
    "    StructField(\"country\",StringType(),True), \\\n",
    "    StructField(\"local_phone\", StringType(), True), \\\n",
    "  ])\n",
    "\n",
    "df = spark.createDataFrame(data=input_data,schema=schema)\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n",
    "# slide 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822f5bb1-a029-4ae5-a942-7dc423dd3335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing CSV file\n",
    "\n",
    "output_path1 = \"../data/csv_example/many_files\"\n",
    "output_path2 = \"../data/csv_example/one_file\"\n",
    "\n",
    "# Write the DataFrame to CSV\n",
    "df.write.mode('overwrite').csv(output_path1, header=True)\n",
    "df.coalesce(1).write.mode('overwrite').csv(output_path2, header=True)\n",
    "\n",
    "# slide 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9807637-49c0-49f4-8bcd-2f55598797d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading CSV file\n",
    "\n",
    "read_path = '../data/csv_example/one_file'\n",
    "\n",
    "print('+ Read csv from file: \\n')\n",
    "df_from_csv = spark.read.csv(read_path)\n",
    "df_from_csv.printSchema()\n",
    "df_from_csv.show()\n",
    "\n",
    "print('+ Read csv with header from file: \\n')\n",
    "df_from_csv = spark.read.csv(read_path,header=True)\n",
    "df_from_csv.printSchema()\n",
    "df_from_csv.show()\n",
    "\n",
    "print('+ Read csv with header and infering data types from file: \\n')\n",
    "df_from_csv = spark.read.csv(read_path,header=True, inferSchema=True)\n",
    "df_from_csv.printSchema()\n",
    "df_from_csv.show()\n",
    "\n",
    "# slide 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b44e50b-d88e-424d-9ed3-0c662915e4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving JSON file\n",
    "output_path1 = \"./data/json_example/many_files\"\n",
    "output_path2 = \"./data/json_example/one_file\"\n",
    "\n",
    "# Write the DataFrame to CSV\n",
    "df.write.mode('overwrite').json(output_path1)\n",
    "df.coalesce(1).write.mode('overwrite').option(\"multiline\",\"true\").json(output_path2)\n",
    "\n",
    "# slide 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfec3108-b62f-4f0b-a07c-3848d662d61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from json\n",
    "\n",
    "read_path = \"./data/json_example/many_files\"\n",
    "\n",
    "print('+ Read json from file: \\n')\n",
    "df_from_csv = spark.read.json(read_path)\n",
    "df_from_csv.printSchema()\n",
    "df_from_csv.show()\n",
    "\n",
    "# slide 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f369fc-99ba-422d-a11b-601f6af8b830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write in DB\n",
    "# You need to have a created DB on a local/remote postgresql server\n",
    "\n",
    "# Define database connection properties\n",
    "db_properties = {\n",
    "    \"url\": \"jdbc:postgresql://localhost:5432/pyspark_db\",\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "    \"user\": \"pyspark\",\n",
    "    \"password\": \"password\",\n",
    "}\n",
    "\n",
    "table_name = \"nice_guys\"\n",
    "\n",
    "# Write the DataFrame to the database\n",
    "df.write \\\n",
    "    .jdbc(url= db_properties[\"url\"],\n",
    "          table=table_name,\n",
    "          mode=\"overwrite\",  # You can use \"append\" or \"ignore\" as well\n",
    "          properties=db_properties)\n",
    "\n",
    "# slide 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc06857-72e2-4ffd-9fe4-0cba3c09077d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read DF from databse\n",
    "\n",
    "# Define database connection properties\n",
    "db_properties = {\n",
    "    \"url\": \"jdbc:postgresql://localhost:5432/pyspark_db\",\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "    \"user\": \"pyspark\",\n",
    "    \"password\": \"password\",\n",
    "}\n",
    "\n",
    "table_name = \"nice_guys\"\n",
    "\n",
    "df_from_postgresql = spark.read \\\n",
    "    .jdbc(url=db_properties[\"url\"],\n",
    "          table=\"nice_guys\",\n",
    "          properties=db_properties)\n",
    "\n",
    "df_from_postgresql.show()\n",
    "df = df_from_postgresql\n",
    "\n",
    "# slide 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff0c530-f9de-4afb-b4b6-214bccceeae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load table with HIVE format to spark catalog to use it as view\n",
    "\n",
    "warehouse_location = spark.conf.get(\"spark.sql.warehouse.dir\")\n",
    "table_name = \"example_table\"\n",
    "\n",
    "df.write.mode('overwrite').parquet('./data/parquet_example')\n",
    "\n",
    "# Load table\n",
    "df = spark.read.format('parquet').load('./data/parquet_example')\n",
    "df.show()\n",
    "\n",
    "# slide 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7530c62c-07e9-43d0-a358-2f2dcaea5423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add table to spark catalog\n",
    "\n",
    "access_table_name = 'my_table'\n",
    "df.createOrReplaceTempView(access_table_name)\n",
    "\n",
    "print('Available tables:')\n",
    "tables = spark.catalog.listTables()\n",
    "for table in tables:\n",
    "    print('+ ',table.name)\n",
    "\n",
    "# slide 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c76337-335d-41c9-88aa-f7505da17653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read some_table if its loaded in the spark.catalog\n",
    "\n",
    "df = spark.read.table(access_table_name)\n",
    "df.show()\n",
    "\n",
    "# slide 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e74fce3-94c4-4630-9644-07894ea2bc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SQL on DF\n",
    "\n",
    "access_table_name = 'my_table'\n",
    "result = spark.sql(f\"SELECT * FROM {access_table_name} WHERE Age >= 45\")\n",
    "result.show()\n",
    "\n",
    "# slide 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ef89b4-f8f5-45b1-8488-83bd2a8d9fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.filter(col('age') < 45).show()\n",
    "\n",
    "# slide 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f772d0-df7a-4004-8899-d930bb9ab623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order by\n",
    "\n",
    "df.orderBy('id').show()\n",
    "\n",
    "df.orderBy('age').show()\n",
    "\n",
    "# slide 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2108b8-2e6c-41a2-ae3b-6acf4e39fe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop\n",
    "\n",
    "df.drop('name').show()\n",
    "\n",
    "# slide 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfb235e-c06a-48f9-b963-5c6939b260a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column to DF\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "new_df = df.withColumn('young', col('age') > 30)\n",
    "new_df.show()\n",
    "\n",
    "# SQL equivalent...\n",
    "access_table_name = 'my_table'\n",
    "spark.sql(f\"SELECT *,(Age > 30) AS young FROM {access_table_name}\").show()\n",
    "\n",
    "# slide 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480f51fc-f2bc-41cc-a695-8df27f0e2ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"id\",\"name\",\"surname\",\"age\",\"country\",\"local_phone\",\"phone_code\"]\n",
    "input_data = [(1,\"Simón\",\"Bolivar\",47,\"VEN\",\"489 895 965\",\"+58\"),\n",
    "    (2,\"Fidel\",\"Castro\",90,\"CU\",\"956 268 348\",\"+53\"),\n",
    "    (3,\"Jose\",\"Doroteo\",45,\"MEX\",\"985 621 444\",\"+52\"),\n",
    "    (4,\"Ernesto\",\"Guevara\",39,\"AR\",\"895 325 481\",\"+54\"),\n",
    "    (5,\"Hugo\",\"Chávez\",58,\"VE\",\"489 895 965\",\"+58\"),\n",
    "    (6,\"Camilo\",\"Cienfuegos\",27,\"CUB\",\"956 268 348\",\"+53\"),\n",
    "    (7,\"Emiliano\",\"Zapata\",39,\"ME\",\"985 621 444\",\"+52\"),\n",
    "    (8,\"Juan Domingo\",\"Perón\",78,\"ARG\",\"985 621 444\",\"+54\"),\n",
    "  ]\n",
    "\n",
    "mod_df = spark.createDataFrame(input_data).toDF(*columns)\n",
    "mod_df.show()\n",
    "\n",
    "# slide 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be794c1c-b67c-4a40-93aa-8938fe92a7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping and Aggregation\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "result = mod_df.groupBy(\"phone_code\") \\\n",
    "                .agg(count(\"*\").alias(\"item_count\"))\n",
    "result.show()\n",
    "\n",
    "# slide 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9537a83-399c-452b-ab8f-bc450d8a3994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping and Aggregation 2 columns\n",
    "\n",
    "from pyspark.sql.functions import collect_list, count\n",
    "\n",
    "result = mod_df.groupBy(\"phone_code\") \\\n",
    "                .agg(count(\"*\").alias(\"item_count\"), \\\n",
    "                     collect_list(\"id\").alias(\"ids\"))\n",
    "result.show()\n",
    "\n",
    "# slide 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b700f520-5d50-4e5c-82ed-78d221fd8353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average\n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "result = mod_df.groupBy(\"phone_code\") \\\n",
    "                .agg(avg('age').alias(\"age_avg\"))\n",
    "\n",
    "result.show()\n",
    "\n",
    "# slide 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f24d92-a18c-4dfe-8a7a-16373956bc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect list, collect set\n",
    "\n",
    "from pyspark.sql.functions import collect_list, collect_set, col\n",
    "\n",
    "df = mod_df.withColumn('young', col('age') > 30)\n",
    "df.show()\n",
    "\n",
    "\n",
    "result = df.groupBy(\"young\") \\\n",
    "                .agg(\n",
    "                      collect_list(\"phone_code\").alias(\"phone_codes\"))\n",
    "result.show(truncate=False)\n",
    "\n",
    "result = df.groupBy(\"young\") \\\n",
    "                .agg(\n",
    "                      collect_set(\"phone_code\").alias(\"phone_codes\"))\n",
    "result.show(truncate=False)\n",
    "\n",
    "\n",
    "# slide 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6710ac-2ea5-4f84-8506-bed29ef74d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode example\n",
    "\n",
    "from pyspark.sql.functions import explode, collect_list, count\n",
    "\n",
    "result = mod_df.groupBy(\"phone_code\") \\\n",
    "                .agg(count(\"*\").alias(\"item_count\"), \\\n",
    "                     collect_list(\"name\").alias(\"names\"))\n",
    "result.show()\n",
    "\n",
    "result = result.select('phone_code', explode('names').alias('name'))\n",
    "\n",
    "result.show()\n",
    "\n",
    "\n",
    "# slide 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7f8dff-c737-44c9-9ab3-790bf08657e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timestamps\n",
    "\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "df=spark.createDataFrame(\n",
    "        data = [ (\"1\",\"2023-09-23 05:23:02.013\")],\n",
    "        schema=[\"id\",\"event\"])\n",
    "df.printSchema()\n",
    "\n",
    "#Timestamp String to DateType\n",
    "df.withColumn(\"timestamp\",to_timestamp(\"event\")) \\\n",
    "  .show()\n",
    "\n",
    "# format 'yyyy-MM-dd  HH:mm:ss.SSS'\n",
    "\n",
    "# Custom format\n",
    "df=spark.createDataFrame(\n",
    "        data = [ (\"1\",\"23-09-2023 05:23:02.013\")],\n",
    "        schema=[\"id\",\"event\"])\n",
    "df.printSchema()\n",
    "\n",
    "#Timestamp String to DateType\n",
    "df.withColumn(\"timestamp\",to_timestamp(\"event\",'dd-MM-yyyy HH:mm:ss.SSSS')) \\\n",
    "  .show()\n",
    "\n",
    "# Custom format, be careful things can go wrong\n",
    "df=spark.createDataFrame(\n",
    "        data = [ (\"1\",\"23-09-2023 05:23:02.013\")],\n",
    "        schema=[\"id\",\"event\"])\n",
    "df.printSchema()\n",
    "\n",
    "#Timestamp String to DateType\n",
    "df.withColumn(\"timestamp\",to_timestamp(\"event\",'MM-dd-yyyy HH:mm:ss.SSSS')) \\\n",
    "  .show()\n",
    "\n",
    "# slide 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e75e586-98de-49f0-a771-c571c4020e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time differences\n",
    "\n",
    "from pyspark.sql.functions import current_date, datediff, col\n",
    "\n",
    "data = [(\"1\",\"2023-09-28\"),(\"2\",\"2001-09-11\"),(\"3\",\"1989-11-09\")]\n",
    "df=spark.createDataFrame(data=data,schema=[\"id\",\"date\"])\n",
    "\n",
    "df.select(\n",
    "      col(\"date\"),\n",
    "      current_date().alias(\"current_date\"),\n",
    "      datediff(current_date(),col(\"date\")).alias(\"datediff\")\n",
    "    ).show()\n",
    "\n",
    "#slide 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff22e3a-bbf1-43fc-8794-4da29c3988e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting\n",
    "\n",
    "from pyspark.sql.functions import asc_nulls_first\n",
    "\n",
    "result = mod_df.orderBy(mod_df.age.asc_nulls_first())\n",
    "result.show()\n",
    "# You will get null values first\n",
    "\n",
    "#slide 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1494f5e-1631-4103-ae91-bdd25deb1979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using regular expersions\n",
    "\n",
    "from pyspark.sql.functions import regexp_extract, regexp_replace, concat_ws, upper\n",
    "\n",
    "prefix_pattern = r'(\\d+)'\n",
    "# Use regexp_extract to extract the prefix from the \"local_phone\" column\n",
    "df1 = mod_df.withColumn(\"phone_code\", regexp_extract(col(\"phone_code\"), prefix_pattern, 1))\n",
    "df1.show()\n",
    "\n",
    "df2 = mod_df.withColumn(\"local_phone\", regexp_replace(col(\"local_phone\"), r'\\s+', ''))\n",
    "df2.show()\n",
    "\n",
    "df3 = mod_df.withColumn(\"phone_number\", concat_ws(\"\", regexp_extract(col(\"phone_code\"), prefix_pattern, 1), \n",
    "                                                  regexp_replace(col(\"local_phone\"), r'\\s+', ''))) \\\n",
    "            .drop('phone_code','local_phone') \\\n",
    "            .withColumn(\"name\", upper(col(\"name\"))) \\\n",
    "            .withColumn(\"surname\", upper(col(\"surname\")))\n",
    "df3.show()\n",
    "\n",
    "\n",
    "#slide 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193113b4-203a-45f5-a409-5fe83eb467b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @udf(returnType=StringType())\n",
    "def is_even_udf(x):\n",
    "    return x % 2 == 0\n",
    "\n",
    "df = spark.range(1, 100).toDF(\"x\")\n",
    "\n",
    "# is_even_udf = udf(is_even_udf, StringType())\n",
    "\n",
    "result_df = df.select(col(\"x\"), is_even_udf(col(\"x\")) \\\n",
    ".alias(\"is_even\"))\n",
    "\n",
    "result_df.printSchema()\n",
    "\n",
    "#slide 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f75a0a-5ed1-4060-a0bf-0a1fbb8adb7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# UDAF, using pandas...\n",
    "\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Define a custom UDAF to calculate the average age per country\n",
    "@pandas_udf(DoubleType(), PandasUDFType.GROUPED_AGG)\n",
    "def average_age_udaf(v):\n",
    "    return v.mean()\n",
    "\n",
    "# Use the UDAF to calculate average age per country\n",
    "result = df.groupBy(\"country\").agg(average_age_udaf(col(\"age\")).alias(\"average_age\"))\n",
    "\n",
    "# Show the result\n",
    "result.show()\n",
    "\n",
    "# slide 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e8359d-1061-4916-a4cc-d7f13f01030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
