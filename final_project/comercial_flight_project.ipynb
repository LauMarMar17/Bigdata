{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Practical Work\n",
    "\n",
    "We are supposed to create a model capable of predicting the arrival delay time of a commercial flight based on several parameters known at the take-off time. Tasks:\n",
    "* Load the input data, previously stored at a known location.\n",
    "* Select, process and transform the input variables, to prepare them for training the model.\n",
    "* Perform some basic analysis of each input variable. \n",
    "* Create a ML model that predicts the arrival delay time.\n",
    "* Validate the created model and provide some measures of its accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/dslab/workspaces/rrunix/spark/final_project'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract files into csv formats\n",
    "import bz2\n",
    "# extract every bz2 format file in ../BigData/data/project_data/ to csv file\n",
    "files = os.listdir(\"../BigData/data/project_data/\")\n",
    "def bz2_to_csv(files):\n",
    "\tfor file in files:\n",
    "\t\tif file.endswith(\".bz2\"):\n",
    "\t\t\tfile_path = \"../BigData/data/project_data/\" + file\n",
    "\t\t\twith bz2.open(file_path, \"rb\") as f:\n",
    "\t\t\t\tfile_content = f.read()\n",
    "\t\t\twith open(\"../BigData/data/project_data/\" + file[:-4], \"wb\") as f:\n",
    "\t\t\t\tf.write(file_content)\n",
    "\n",
    "bz2_to_csv(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Version: 3.5.0\n"
     ]
    }
   ],
   "source": [
    "# Create a SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(\"local\", \"ComercialFlights\")\n",
    "spark = SparkSession.builder \\\n",
    "            .appName(\"First Session\") \\\n",
    "            .master(\"local[*]\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "print(\"Spark Version: {}\".format(sc.version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.                 (0 + 1) / 1]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/dslab/anaconda3/envs/spark/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/dslab/anaconda3/envs/spark/lib/python3.8/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/home/dslab/anaconda3/envs/spark/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \t\tdf_pyspark\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[1;32m     10\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m df_pyspark\n\u001b[0;32m---> 12\u001b[0m df_pyspark \u001b[38;5;241m=\u001b[39m \u001b[43mcsv_to_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_files\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# unir todos los dataframes en uno solo\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m reduce\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mcsv_to_df\u001b[0;34m(csv_files)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m csv_files:\n\u001b[1;32m      7\u001b[0m \tfile_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../BigData/data/project_data/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m file\n\u001b[0;32m----> 8\u001b[0m \tdf \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minferSchema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \tdf_pyspark\u001b[38;5;241m.\u001b[39mappend(df)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df_pyspark\n",
      "File \u001b[0;32m~/anaconda3/envs/spark/lib/python3.8/site-packages/pyspark/sql/readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m~/anaconda3/envs/spark/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/anaconda3/envs/spark/lib/python3.8/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/anaconda3/envs/spark/lib/python3.8/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/spark/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Use DataFrames to read csv files\n",
    "# read all csv files in ../BigData/data/project_data/ to pyspark dataframe\n",
    "csv_files = os.listdir(\"../BigData/data/project_data/\")\n",
    "def csv_to_df(csv_files):\n",
    "\tdf_pyspark =[]\n",
    "\tfor file in csv_files:\n",
    "\t\tfile_path = \"../BigData/data/project_data/\" + file\n",
    "\t\tdf = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\t\tdf_pyspark.append(df)\n",
    "\treturn df_pyspark\n",
    "\n",
    "df_pyspark = csv_to_df(csv_files)\n",
    "\n",
    "# unir todos los dataframes en uno solo\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "def unionAll(*dfs):\n",
    "\treturn reduce(DataFrame.unionAll, dfs)\n",
    "\n",
    "df = unionAll(*df_pyspark)\n",
    "df.show(5)\n",
    "\n",
    "\t\t\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:===================================================>  (131 + 6) / 137]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25952068\n",
      "29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# comprobar el numero de filas de df \n",
    "print(df.count())\n",
    "# comprobar el numero de columnas de df\n",
    "print(len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has 29 columns. We won't use all of them. The ones that should be droped are: \n",
    "* ArrTime\n",
    "* ActualElapsedTime\n",
    "* AirTime\n",
    "* TaxiIn\n",
    "* Diverted\n",
    "* CarrierDelay\n",
    "* WeatherDelay\n",
    "* NASDelay\n",
    "* SecurityDelay\n",
    "* LateAircraftDelay\n",
    "\n",
    "Meaning of the variables: \n",
    "1. Year 1987-2008 \n",
    "2. Month 1-12 \n",
    "3. DayofMonth 1-31 \n",
    "4. DayOfWeek 1 (Monday) - 7 (Sunday)\n",
    "5. DepTime actual departure time (local, hhm m) \n",
    "6. CRSDepTime scheduled departure time (local, hhmm) \n",
    "7. CRSArrTime scheduled arrival time (local, hhmm) \n",
    "8. UniqueCarrier Airline code \n",
    "9. FlightNum flight number \n",
    "10. TailNum plane tail number \n",
    "11. CRSElapsedTime in minutes (estimated flight time)\n",
    "12. ArrDelay arrival delay, in minutes -- TARGET VARIABLE\n",
    "13. DepDelay departure delay, in minutes \n",
    "14. Origin origin IATA airport code \n",
    "15. Dest destination IATA airport code \n",
    "16. Distance in miles \n",
    "17. TaxiOut taxi out time in minutes (tiempo que tarda el avión desde la puerta de embarque hasta el despegue\")\n",
    "18. Cancelled was the flight cancelled? \n",
    "19. CancellationCode reason for cancellation (A = carrier, B = weather, C = NAS, D = security) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "# rename columns:\n",
    "def edit_column_names(df):\n",
    "    df =  df.withColumnRenamed('DayofMonth','day_of_month').\\\n",
    "                withColumnRenamed('DayOfWeek','day_of_week').\\\n",
    "                withColumnRenamed('DepTime','actual_departure_time').\\\n",
    "                withColumnRenamed('CRSDepTime','scheduled_departure_time').\\\n",
    "                withColumnRenamed('ArrTime','actual_arrival_time').\\\n",
    "                withColumnRenamed('CRSArrTime','scheduled_arrival_time').\\\n",
    "                withColumnRenamed('UniqueCarrier','airline_code').\\\n",
    "                withColumnRenamed('FlightNum','flight_number').\\\n",
    "                withColumnRenamed('TailNum','plane_number').\\\n",
    "                withColumnRenamed('ActualElapsedTime','actual_flight_time').\\\n",
    "                withColumnRenamed('CRSElapsedTime','scheduled_flight_time').\\\n",
    "                withColumnRenamed('AirTime','air_time').\\\n",
    "                withColumnRenamed('ArrDelay','arrival_delay').\\\n",
    "                withColumnRenamed('DepDelay','departure_delay').\\\n",
    "                withColumnRenamed('TaxiIn','taxi_in').\\\n",
    "                withColumnRenamed('TaxiOut','taxi_out').\\\n",
    "                withColumnRenamed('CancellationCode','cancellation_code').\\\n",
    "                withColumnRenamed('CarrierDelay','carrier_delay').\\\n",
    "                withColumnRenamed('WeatherDelay','weather_delay').\\\n",
    "                withColumnRenamed('NASDelay','nas_delay').\\\n",
    "                withColumnRenamed('SecurityDelay','security_delay').\\\n",
    "                withColumnRenamed('LateAircraftDelay','late_aircraft_delay')\n",
    "    for col in df.columns:\n",
    "        df = df.withColumnRenamed(col, col.lower())\n",
    "    return df\n",
    "\n",
    "# select columns:\n",
    "def my_columns (df):\n",
    "    df = df.select('year','month','day_of_month', 'day_of_week', 'actual_departure_time', 'scheduled_departure_time', 'scheduled_arrival_time', 'airline_code', 'flight_number', 'plane_number', 'scheduled_flight_time', 'arrival_delay', 'departure_delay', 'origin', 'dest', 'distance', 'taxi_out', 'cancelled', 'cancellation_code')\n",
    "    return df\n",
    "\n",
    "# convert days to names:\n",
    "def convert_days_to_names(df):\n",
    "    df = df.withColumn('day_of_week', when(df.day_of_week == 1,'Monday').\\\n",
    "                                              when(df.day_of_week ==2,'Tuesday').\\\n",
    "                                              when(df.day_of_week ==3,'Wednesday').\\\n",
    "                                              when(df.day_of_week ==4,'Thursday').\\\n",
    "                                              when(df.day_of_week ==5,'Friday').\\\n",
    "                                              when(df.day_of_week ==6,'Saturday').\\\n",
    "                                              when(df.day_of_week ==7,'Sunday'))\n",
    "    return df\n",
    "\n",
    "# combine to create dates:\n",
    "def add_date_column(df):\n",
    "    df = df.withColumn('date', to_date(concat(col('day_of_month'), lit(' '), col('month'), lit(' '), col('year')), 'd M yyyy'))\n",
    "    return df\n",
    "\n",
    "# convert time to minutes:\n",
    "def convert_time_to_minutes(df):\n",
    "    # Define the external function\n",
    "    def my_function(value)->int:\n",
    "        value = str(value)\n",
    "        if not value.isnumeric():\n",
    "            return None\n",
    "        value = str(int(value)) \n",
    "        mins = 0\n",
    "        if value:\n",
    "            if len(value) == 4:\n",
    "                mins = int(value[:2])*60+int(value[2:])\n",
    "            elif len(value) == 3:\n",
    "                mins = int(value[:1])*60 + int(value[1:])\n",
    "            return mins\n",
    "        else:\n",
    "            None\n",
    "    # Register the UDF\n",
    "    my_udf = udf(my_function, IntegerType())\n",
    "    df = df.withColumn('actual_departure_time',my_udf(col('actual_departure_time')))\\\n",
    "                       .withColumn('scheduled_departure_time',my_udf(col('scheduled_departure_time')))\\\n",
    "                       .withColumn('actual_arrival_time',my_udf(col('actual_arrival_time')))\\\n",
    "                       .withColumn('scheduled_arrival_time',my_udf(col('scheduled_arrival_time')))\n",
    "    return df\n",
    "\n",
    "# standarize df\n",
    "def standarize_dataframe(df):\n",
    "    temp = edit_column_names(df)\n",
    "    temp = my_columns(temp)\n",
    "    temp = convert_days_to_names(temp)\n",
    "    temp = add_date_column(temp)\n",
    "    temp = convert_time_to_minutes(temp)\n",
    "    return temp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------+-----------+---------------------+------------------------+-------------------+----------------------+------------+-------------+------------+------------------+---------------------+--------+-------------+---------------+------+----+--------+-------+--------+---------+-----------------+--------+-------------+-------------+---------+--------------+-------------------+----------+\n",
      "|year|month|day_of_month|day_of_week|actual_departure_time|scheduled_departure_time|actual_arrival_time|scheduled_arrival_time|airline_code|flight_number|plane_number|actual_flight_time|scheduled_flight_time|air_time|arrival_delay|departure_delay|origin|dest|distance|taxi_in|taxi_out|cancelled|cancellation_code|diverted|carrier_delay|weather_delay|nas_delay|security_delay|late_aircraft_delay|      date|\n",
      "+----+-----+------------+-----------+---------------------+------------------------+-------------------+----------------------+------------+-------------+------------+------------------+---------------------+--------+-------------+---------------+------+----+--------+-------+--------+---------+-----------------+--------+-------------+-------------+---------+--------------+-------------------+----------+\n",
      "|1988|    1|           9|   Saturday|                  856|                     829|               1458|                   896|          PI|          942|          NA|                70|                   64|      NA|           23|             17|   SYR| BWI|     273|     NA|      NA|        0|               NA|       0|           NA|           NA|       NA|            NA|                 NA|1988-01-09|\n",
      "|1988|    1|          10|     Sunday|                  834|                     829|               1443|                   896|          PI|          942|          NA|                69|                   64|      NA|            8|              3|   SYR| BWI|     273|     NA|      NA|        0|               NA|       0|           NA|           NA|       NA|            NA|                 NA|1988-01-10|\n",
      "|1988|    1|          11|     Monday|                  913|                     829|               1553|                   896|          PI|          942|          NA|                67|                   64|      NA|           78|             75|   SYR| BWI|     273|     NA|      NA|        0|               NA|       0|           NA|           NA|       NA|            NA|                 NA|1988-01-11|\n",
      "|1988|    1|          12|    Tuesday|                  834|                     829|               1438|                   896|          PI|          942|          NA|                64|                   64|      NA|            3|              3|   SYR| BWI|     273|     NA|      NA|        0|               NA|       0|           NA|           NA|       NA|            NA|                 NA|1988-01-12|\n",
      "|1988|    1|          13|  Wednesday|                  845|                     829|               1503|                   896|          PI|          942|          NA|                82|                   64|      NA|           28|             10|   SYR| BWI|     273|     NA|      NA|        0|               NA|       0|           NA|           NA|       NA|            NA|                 NA|1988-01-13|\n",
      "+----+-----+------------+-----------+---------------------+------------------------+-------------------+----------------------+------------+-------------+------------+------------------+---------------------+--------+-------------+---------------+------+----+--------+-------+--------+---------+-----------------+--------+-------------+-------------+---------+--------------+-------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df = standarize_dataframe(df)\n",
    "new_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "# Filter data: We won't use all the columns, so we can drop them out\n",
    "my_df = df.select(\"Year\", \"Month\", \"DayofMonth\", \"DayOfWeek\", \"DepTime\", \"CRSDepTime\", \"CRSArrTime\", \"UniqueCarrier\", \"FlightNum\", \"TailNum\", \"CRSElapsedTime\", \"ArrDelay\", \"DepDelay\", \"Origin\", \"Dest\", \"Distance\", \"TaxiOut\", \"Cancelled\", \"CancellationCode\")\n",
    "\n",
    "# print number of columns in df_pyspark and my_df\n",
    "print(len(df.columns))\n",
    "print(len(my_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: string (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- CRSElapsedTime: string (nullable = true)\n",
      " |-- ArrDelay: string (nullable = true)\n",
      " |-- DepDelay: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: string (nullable = true)\n",
      " |-- TaxiOut: string (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------+----------+----------+-------------+---------+-------+--------------+--------+--------+------+----+--------+-------+---------+----------------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|CRSElapsedTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiOut|Cancelled|CancellationCode|\n",
      "+----+-----+----------+---------+-------+----------+----------+-------------+---------+-------+--------------+--------+--------+------+----+--------+-------+---------+----------------+\n",
      "|1988|    1|         9|        6|   1348|      1331|      1435|           PI|      942|     NA|            64|      23|      17|   SYR| BWI|     273|     NA|        0|              NA|\n",
      "|1988|    1|        10|        7|   1334|      1331|      1435|           PI|      942|     NA|            64|       8|       3|   SYR| BWI|     273|     NA|        0|              NA|\n",
      "|1988|    1|        11|        1|   1446|      1331|      1435|           PI|      942|     NA|            64|      78|      75|   SYR| BWI|     273|     NA|        0|              NA|\n",
      "|1988|    1|        12|        2|   1334|      1331|      1435|           PI|      942|     NA|            64|       3|       3|   SYR| BWI|     273|     NA|        0|              NA|\n",
      "|1988|    1|        13|        3|   1341|      1331|      1435|           PI|      942|     NA|            64|      28|      10|   SYR| BWI|     273|     NA|        0|              NA|\n",
      "+----+-----+----------+---------+-------+----------+----------+-------------+---------+-------+--------------+--------+--------+------+----+--------+-------+---------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:=====================================================>(136 + 1) / 137]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------+----------+----------+-------------+---------+-------+--------------+--------+--------+------+----+--------+-------+---------+----------------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|CRSArrTime|UniqueCarrier|FlightNum|TailNum|CRSElapsedTime|ArrDelay|DepDelay|Origin|Dest|Distance|TaxiOut|Cancelled|CancellationCode|\n",
      "+----+-----+----------+---------+-------+----------+----------+-------------+---------+-------+--------------+--------+--------+------+----+--------+-------+---------+----------------+\n",
      "|   0|    0|         0|        0|      0|         0|         0|            0|        0|  84904|             0|       0|       0|     0|   0|       0|      0|        0|         4649550|\n",
      "+----+-----+----------+---------+-------+----------+----------+-------------+---------+-------+--------------+--------+--------+------+----+--------+-------+---------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# contar número de variables nulas en cada columna\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "my_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in my_df.columns]).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no hay valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:=======================================================>(78 + 1) / 79]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+---------+-------+----------+----------+-------------+---------+--------+--------------+--------+--------+------+----+--------+--------+---------+----------------+\n",
      "|Year|Month|DayofMonth|DayOfWeek|DepTime|CRSDepTime|CRSArrTime|UniqueCarrier|FlightNum| TailNum|CRSElapsedTime|ArrDelay|DepDelay|Origin|Dest|Distance| TaxiOut|Cancelled|CancellationCode|\n",
      "+----+-----+----------+---------+-------+----------+----------+-------------+---------+--------+--------------+--------+--------+------+----+--------+--------+---------+----------------+\n",
      "|   0|    0|         0|        0| 139696|         0|         0|            0|        0|13027844|             0|  176198|  139696|     0|   0|   24234|13027844|        0|        13027844|\n",
      "+----+-----+----------+---------+-------+----------+----------+-------------+---------+--------+--------------+--------+--------+------+----+--------+--------+---------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# contar el número de variables nan en cada columna\n",
    "my_df.select([count(when(col(c) == \"NA\", c)).alias(c) for c in my_df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 51:=====================================================>(136 + 1) / 137]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+\n",
      "|UniqueCarrier|TailNum|\n",
      "+-------------+-------+\n",
      "|           9E|   NULL|\n",
      "|           AA|     NA|\n",
      "|           AQ| N836AL|\n",
      "|           AS|     NA|\n",
      "|           B6| N281JB|\n",
      "+-------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# los valores nulos de \"TailNum\" cambiar por el valor más repetido en esa aerolínea (UniqueCarrier)\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank, col\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "# para cada aerolínea, calcular el valor más repetido en la columna \"TailNum\"\n",
    "w = Window.partitionBy(\"UniqueCarrier\").orderBy(col(\"count\").desc())\n",
    "df_tailnum = my_df.groupBy(\"UniqueCarrier\", \"TailNum\").count().withColumn(\"rank\", rank().over(w)).filter(col(\"rank\") == 1).drop(\"rank\", \"count\")\n",
    "df_tailnum.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
